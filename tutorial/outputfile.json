{"title": ["Scrapy 2.8 documentation \u2014 Scrapy 2.8.0 documentation"], "image": [], "url": ["#", "intro/overview.html", "intro/install.html", "intro/tutorial.html", "intro/examples.html", "topics/commands.html", "topics/spiders.html", "topics/selectors.html", "topics/items.html", "topics/loaders.html", "topics/shell.html", "topics/item-pipeline.html", "topics/feed-exports.html", "topics/request-response.html", "topics/link-extractors.html", "topics/settings.html", "topics/exceptions.html", "topics/logging.html", "topics/stats.html", "topics/email.html", "topics/telnetconsole.html", "faq.html", "topics/debug.html", "topics/contracts.html", "topics/practices.html", "topics/broad-crawls.html", "topics/developer-tools.html", "topics/dynamic-content.html", "topics/leaks.html", "topics/media-pipeline.html", "topics/deploy.html", "topics/autothrottle.html", "topics/benchmarking.html", "topics/jobs.html", "topics/coroutines.html", "topics/asyncio.html", "topics/architecture.html", "topics/downloader-middleware.html", "topics/spider-middleware.html", "topics/extensions.html", "topics/signals.html", "topics/scheduler.html", "topics/exporters.html", "topics/components.html", "topics/api.html", "news.html", "contributing.html", "versioning.html", "#", "#", "https://github.com/scrapy/scrapy/blob/2.8/docs/index.rst", "#scrapy-version-documentation", "https://en.wikipedia.org/wiki/Web_crawler", "https://en.wikipedia.org/wiki/Web_scraping", "#getting-help", "faq.html", "genindex.html", "py-modindex.html", "https://stackoverflow.com/tags/scrapy", "https://www.reddit.com/r/scrapy/", "https://groups.google.com/forum/#!forum/scrapy-users", "irc://irc.freenode.net/scrapy", "https://github.com/scrapy/scrapy/issues", "https://discord.gg/mv3yErfpvq", "#first-steps", "intro/overview.html", "intro/install.html", "intro/tutorial.html", "intro/examples.html", "#basic-concepts", "topics/commands.html", "topics/spiders.html", "topics/selectors.html", "topics/shell.html", "topics/items.html", "topics/loaders.html", "topics/item-pipeline.html", "topics/feed-exports.html", "topics/request-response.html", "topics/link-extractors.html", "topics/settings.html", "topics/settings.html#topics-settings-ref", "topics/exceptions.html", "#built-in-services", "topics/logging.html", "topics/stats.html", "topics/email.html", "topics/telnetconsole.html", "#solving-specific-problems", "faq.html", "topics/debug.html", "topics/contracts.html", "topics/practices.html", "topics/broad-crawls.html", "topics/developer-tools.html", "topics/dynamic-content.html", "topics/leaks.html", "topics/media-pipeline.html", "topics/deploy.html", "topics/autothrottle.html", "topics/benchmarking.html", "topics/jobs.html", "topics/coroutines.html", "https://docs.python.org/3/reference/compound_stmts.html#async", "topics/asyncio.html", "https://docs.python.org/3/library/asyncio.html#module-asyncio", "https://docs.python.org/3/library/asyncio.html#module-asyncio", "#extending-scrapy", "topics/architecture.html", "topics/downloader-middleware.html", "topics/spider-middleware.html", "topics/extensions.html", "topics/signals.html", "topics/scheduler.html", "topics/exporters.html", "topics/components.html", "topics/api.html", "#all-the-rest", "news.html", "contributing.html", "versioning.html", "intro/overview.html", "https://www.sphinx-doc.org/", "https://github.com/readthedocs/sphinx_rtd_theme", "https://readthedocs.org", "/en/master/", "/en/latest/", "/en/stable/", "/en/2.8/", "/en/2.7/", "/en/2.6/", "/en/2.5/", "/en/2.4/", "/en/2.3/", "/en/2.2/", "/en/2.1/", "/en/2.0/", "/en/1.8/", "/en/1.7/", "/en/1.6/", "/en/1.5/", "/en/1.4/", "/en/1.3/", "/en/1.2/", "/en/1.1/", "/en/1.0/", "/en/0.24/", "/en/0.22/", "/en/0.20/", "/en/0.18/", "/en/0.16/", "/en/0.14/", "/en/0.12/", "/en/0.10.3/", "/en/0.9/", "/en/xpath-tutorial/", "//docs.scrapy.org/_/downloads/en/latest/pdf/", "//docs.scrapy.org/_/downloads/en/latest/htmlzip/", "//docs.scrapy.org/_/downloads/en/latest/epub/", "//readthedocs.org/projects/scrapy/?fromdocs=scrapy", "//readthedocs.org/builds/scrapy/?fromdocs=scrapy"], "p_tags": ["Scrapy is a fast high-level ", " and ", " framework, used\nto crawl websites and extract structured data from their pages. It can be used\nfor a wide range of purposes, from data mining to monitoring and automated\ntesting.", "Having trouble? We\u2019d like to help!", "Try the ", " \u2013 it\u2019s got answers to some common questions.", "Looking for specific information? Try the ", " or ", ".", "Ask or search questions in ", ".", "Ask or search questions in the ", ".", "Search for questions on the archives of the ", ".", "Ask a question in the ", ",", "Report bugs with Scrapy in our ", ".", "Join the Discord community ", ".", "Understand what Scrapy is and how it can help you.", "Get Scrapy installed on your computer.", "Write your first Scrapy project.", "Learn more by playing with a pre-made Scrapy project.", "Learn about the command-line tool used to manage your Scrapy project.", "Write the rules to crawl your websites.", "Extract the data from web pages using XPath.", "Test your extraction code in an interactive environment.", "Define the data you want to scrape.", "Populate your items with the extracted data.", "Post-process and store your scraped data.", "Output your scraped data using different formats and storages.", "Understand the classes used to represent HTTP requests and responses.", "Convenient classes to extract links to follow from pages.", "Learn how to configure Scrapy and see all ", ".", "See all available exceptions and their meaning.", "Learn how to use Python\u2019s builtin logging on Scrapy.", "Collect statistics about your scraping crawler.", "Send email notifications when certain events occur.", "Inspect a running crawler using a built-in Python console.", "Get answers to most frequently asked questions.", "Learn how to debug common problems of your Scrapy spider.", "Learn how to use contracts for testing your spiders.", "Get familiar with some Scrapy common practices.", "Tune Scrapy for crawling a lot domains in parallel.", "Learn how to scrape with your browser\u2019s developer tools.", "Read webpage data that is loaded dynamically.", "Learn how to find and get rid of memory leaks in your crawler.", "Download files and/or images associated with your scraped items.", "Deploying your Scrapy spiders and run them in a remote server.", "Adjust crawl rate dynamically based on load.", "Check how Scrapy performs on your hardware.", "Learn how to pause and resume crawls for large spiders.", "Use the ", ".", "Use ", " and ", "-powered libraries.", "Understand the Scrapy architecture.", "Customize how pages get requested and downloaded.", "Customize the input and output of your spiders.", "Extend Scrapy with your custom functionality", "See all available signals and how to work with them.", "Understand the scheduler component.", "Quickly export your scraped items to a file (XML, CSV, etc).", "Learn the common API and some good practices when building custom Scrapy\ncomponents.", "Use it on extensions and middlewares to extend Scrapy functionality.", "See what has changed in recent Scrapy versions.", "Learn how to contribute to the Scrapy project.", "Understand Scrapy versioning and API stability.", "\u00a9 Copyright 2008\u20132023, Scrapy developers.\n      ", "\n      "], "heading1": ["Scrapy 2.8 documentation"], "heading2": ["Getting help", "First steps", "Basic concepts", "Built-in services", "Solving specific problems", "Extending Scrapy", "All the rest"], "heading3": [], "heading4": [], "heading5": [], "heading6": []}
